\documentclass{article}

\usepackage{./prelude}

\DeclareMathOperator{\Bilin}{Bilin}

\Class{Math 416H}
\Title{HW \#7}
\Author{Your Name}{netid}

\begin{document}

\begin{problem}
Suppose $U$ is a subspace of a finite-dimensional vector space $V$ over some field $F$.
Recall that the \emph{annihilator} $U^\circ$ of $U$ consists of all linear maps $\ell \in V$ which are zero on the vectors in $U$.
\[
	\dim U^\circ + \dim U = \dim V
\]
\end{problem}
\begin{solution}
	Since $V$ is finite-dimensional, $U \subseteq V$ is also finite-dimensional.
	So, construct a basis $\set{u_1, \dots, u_n}$ of $U$ and extend it to a basis $\mathcal{B} = \set{u_1, \dots, u_n, v_1, \dots, v_m}$ of $V$.

	$\mathcal{B}$ induces a dual basis $\mathcal{B}^* = \set{u_1^*, \dots, u_n^*, v_1^*, \dots v_m^*}$ of $V^*$, where $b_i^*(b_j) = \delta_{ij}$ for all $b_i^* \in \mathcal{B}^*, b_j \in \mathcal{B}$.
	It suffices to show that $\dim U^\circ = m$, since then $\dim U^\circ + \dim U = m + n = \dim V$.

	\begin{claim*}
		$\set{v_1^*, \dots, v_m^*}$ is a basis of $U^\circ$.
	\end{claim*}
	Since $\set{v_1^*, \dots, v_m^*} \subseteq \mathcal{B}^*$, we have linear independence.

	Let $\ell \in U^\circ$ and consider the map $\ell' : V \to \R$ given by $\ell'(v) \coloneq \sum \ell(v) v_i^*(v)$.
	Then, for any $b \in \mathcal{B}$, we have two cases:
	\begin{itemize}
		\item If $b = u_i$ for some $i$, then $\ell'(u_i) = 0 = \ell(u_i)$.
		\item If $b = v_i$ for some $i$, then $\ell'(v_i) = \sum \ell(v_i) v_j^*(v_i) = 0 + \dots + 0 + \ell(v_i)v_i^*(v_i) + 0 + \dots + 0 = \ell(v_i)$.
	\end{itemize}
	Since a linear map is uniquely characterized by its behavior on a basis, we conclude that $\ell' = \ell$, so $\set{v_1^*, \dots, v_m^*}$ spans $U^\circ$.
\end{solution}

\begin{problem}
Suppose $T : V \to W$ is a linear map between two finite-dimensional vector spaces and $T^* : W^* \to V^*$ is the dual map.
Prove that
\begin{enumerate}[label=(\alph*)]
	\item \[N(T^*) = (R(T))^\circ\]
	      \begin{solution}
		      Let $\ell \in N(T^*)$, so $\ell \circ T = 0$.
		      Fix $w \in R(T)$ and pick $v \in V$ such that $T(v) = w$.
		      Then, $\ell(w) = \ell(T(v)) = (\ell \circ T)(v) = 0$.
		      Hence, $\ell \in (R(T))^\circ$.

		      Let $\ell \in (R(T))^\circ$, so $\ell(w) = 0$ for all $w \in R(T)$.
		      Fix $v \in V$ and set $w = T(v)$.
		      Then, $(T^*(\ell))(v) = (\ell \circ T)(v) = \ell(w) = 0$.
		      Hence, $\ell \in N(T^*)$.
	      \end{solution}

	\item \[\dim R(T^*) = \dim R(T)\]
	      \begin{solution}
		      Since $W$ is finite-dimensional, $R(T) \subseteq W$ is finite-dimensional.
		      So, we can apply the result from problem 1 to get $\dim (R(T))^\circ + \dim R(T) = \dim W$.
		      It then follows from (a) that $\dim N(T^*) + \dim R(T) = \dim W$.

		      Recall from Rank/Nullity that $\dim R(T^*) = \dim W^* - \dim N(T^*)$.
		      So, by some algebra, we conclude that $\dim W^* - \dim R(T^*) + \dim R(T) = \dim W$.

		      Then, since $\dim W = \dim W^*$, we conclude that $\dim R(T) = \dim R(T^*)$.
	      \end{solution}

	\item for any $m \times n$ matrix $A$, the rank of $A$ equals the rank of its transpose $A^T$.
	      \begin{solution}
		      Let $A = (a_{ij}) \in M_{m, n}(F)$.
		      Then, by Lemma 16.3 from lecture, we have $L_{A^T} = L_A^*$.
		      So, by (b), we conclude that $\dim R(L_A) = \dim R(L_A^*) = \dim R(L_{A^T})$.
	      \end{solution}
\end{enumerate}
\end{problem}

\newpage

\begin{problem}
Assume that $V, W$ are finite-dimensional vector spaces.
Prove that if $T^* : W^* \to V^*$ is injective, then $T: V \to W$ is surjective.
\end{problem}
\begin{solution}
	Since $W$ is finite-dimensional, $R(T) \subseteq W$ is also finite-dimensional.
	So, we apply the result from problem 1 to get $(\dim R(T))^\circ + \dim R(T) = \dim W$.
	Moreover, 2(a) implies that $(R(T))^\circ = N(T^*)$, so we rewrite the left-hand side of the equation to get
	$\dim N(T^*) + \dim R(T) = \dim W$.
	But, since $T^*$ is injective, $\dim N(T^*) = 0$, so we conclude that $\dim R(T) = \dim W$.
	Thus, $T$ is surjective.
\end{solution}

\begin{problem}
Let $V, W$ be two vector spaces.
Prove that the space of bilinear maps
\[
	\Bilin(V, W; \R) \coloneq \set{b : V \times W \to \R \mid \text{$b$ is bilinear}}
\]
is a vector space with scalar multiplication and vector addition defined by
\[
	(\lambda b)(v, w) \coloneq \lambda b(v, w) \text{ for all $\lambda \in \R, v \in V, w \in W$}
\]
and
\[
	(b_1 + b_2)(v, w) \coloneq b_1(v, w) + b_2(v, w) \text{ for all $v \in V, w \in W$.}
\]
\end{problem}
\begin{solution}
	Let $b_1, b_2 \in \Bilin(V, W; \R)$, $\lambda, \mu \in \R$, $v \in V$, and $w \in W$.
	\begin{enumerate}
		\item Addition is commutative:
		      \[(b_1 + b_2)(v, w) = b_1(v, w) + b_2(v, w) = b_2(v, w) + b_1(v, w) = (b_2 + b_1)(v, w)\]
		\item Addition is associative:
		      \begin{multline*}
			      (b_1 + (b_2 + b_3))(v, w) = b_1(v, w) + (b_2 + b_3)(v, w) = b_1(v, w) + (b_2(v, w) + b_3(v, w))   \\
			      = (b_1(v, w) + b_2(v, w)) + b_3(v, w) = (b_1 + b_2)(v, w) + b_3(v, w) = ((b_1 + b_2) + b_3)(v, w)
		      \end{multline*}
		\item The map $0 : V \times W \to \R$, $(v, w) \mapsto 0_{\R}$ behaves like a zero:
		      \[
			      b_1(v, w) + 0(v, w) = b_1(v, w) + 0_\R = b_1(v, w) = 0_\R + b_1(v, w) = 0(v, w) + b_1(v, w)
		      \]
		\item $-b_1 : V \times W \to \R$, $(v, w) \mapsto -b(v, w)$ is the additive inverse of $b_1$:
		      \[
			      (b_1 + -b_1)(v, w) = b_1(v, w) + -b_1(v, w) = b_1(v, w) - b_1(v, w) = 0_\R = 0(v, w)
		      \]
		\item $1 \in \R$ is the multiplicative identity:
		      \[
			      (1 \cdot b_1)(v, w) = 1 \cdot b_1(v, w) = b_1(v, w)
		      \]
		\item Scalar multiplication is associative:
		      \[
			      (\lambda \cdot \mu b_1)(v, w) = \lambda (\mu b_1)(v, w) = (\lambda \mu) b_1(v, w) = ((\lambda \mu) b_1)(v, w)
		      \]
		\item Scalar multiplication distributes over vector addition:
		      \[
			      (\lambda (b_1 + b_2))(v, w) = \lambda (b_1 + b_2)(v, w) = \lambda (b_1(v, w) + b_2(v, w)) = \lambda b_1(v, w) + \lambda b_2(v, w)
		      \]
		\item Scalar multiplication distributes over field addition:
		      \[
			      ((\lambda + \mu)b_1)(v, w) = (\lambda + \mu)b_1(v, w) = \lambda b_1(v, w) + \mu b_1(v, w)
		      \]
	\end{enumerate}
	Hence, $\Bilin(V, W; \R)$ is a vector space.
\end{solution}

\begin{problem}
Let $V, W$ be two vector spaces over $\R$.
Let $b : V \times W \to \R$ be a bilinear map.
\begin{enumerate}[label=(\alph*)]
	\item Prove that for any $w \in W$, the function $w^{\#} : V \to \R$ defined by
	      \[
		      w^{\#}(v) \coloneq b(v, w) \qquad \text{for all $v \in V$} \qquad \text{i.e., $w^{\#}(\cdot) = b(\cdot, w)$}
	      \]
	      is linear.
	      \begin{solution}
		      For all $v, v' \in V, \lambda, \mu \in \R$,
		      \begin{align*}
			      w^{\#}(\lambda v + \mu v') & = b(\lambda v + \mu v', w)                             \\
			                                 & = \lambda b(v, w) + \mu b(v', w) \tag{$b$ is bilinear} \\
			                                 & = \lambda w^{\#}(v) + \mu w^{\#}(v')
		      \end{align*}
		      So, $w^{\#}$ is linear.
	      \end{solution}

	\item Prove that the map
	      \[
		      \# : W \to V^*, \qquad w \mapsto w^{\#}
	      \]
	      is linear.
	      \begin{solution}
		      For all $w, w' \in W, \lambda, \mu \in \R$, we have for every $v \in V$ that
		      \begin{align*}
			      \#(\lambda w + \mu w')(v) & = (\lambda w + \mu w')^{\#}(v)                         \\
			                                & = b(v, \lambda w + \mu w')                             \\
			                                & = \lambda b(v, w) + \mu b(v, w') \tag{$b$ is bilinear} \\
			                                & = \lambda \#(w)(v) + \mu \#(w')(v)
		      \end{align*}
		      So, $\#$ is linear.
	      \end{solution}

	\item Prove that the null space $N(\#)$ of $\#$ is
	      \[
		      S = \set{w \in W \mid b(v, w) = 0 \text{ for all $v \in V$}}
	      \]
	      \begin{solution}
		      Let $w \in W$ be such that $b(v, w) = 0$ for all $v \in V$.
		      Then, for all $v \in V$,
		      \begin{align*}
			      \#(w)(v) & = w^{\#}(v) \\
			               & = b(v, w)   \\
			               & = 0
		      \end{align*}
		      Hence, $\#(w)$ is the zero vector in $V^*$, so $w \in N(\#)$.

		      Let $w \in N(\#)$.
		      Then, $\#(w) = w^{\#} = b(\cdot, w) = 0_{V^*}$.
		      Hence, $(b(\cdot, w))(v) = b(v, w) = 0$, for all $v \in V$, so $w \in S$.
	      \end{solution}

	      \newpage
	\item Now suppose that $W$ is finite-dimensional, $V = W^*$, and $b : W^* \cross W \to \R$ is given by
	      \[
		      b(\ell, w) \coloneq \ell(w)
	      \]
	      Prove that, in this case, $\# : W \to (W^*)^*$ is injective.
	      \begin{solution}
		      Let $w \in N(\#)$ and $\ell \in W^*$.
		      Then, we have $\#(w)(\ell) = w^{\#}(\ell) = b(\ell, w) = 0$.
		      We want to show that $w = 0$.

		      Since $W$ is finite-dimensional, it has a basis $\mathcal{B} = \set{w_1, \dots, w_n}$.
		      So, there are $\lambda_1, \dots, \lambda_n \in \R$ so that $\sum \lambda_i w_i = w$.

		      For each $w_i$, consider the map $w_i^* : W \to \R$ given by $w_i(w_j) \coloneq \delta_{ij}$ for each $w_j \in \mathcal{B}$.
		      Then, $w_i^*(w) = w_i^*(\sum \lambda_j w_j) = 0 + \dots + 0 + w_{i}^*(\lambda_i w_i) + 0 + \dots + 0 = \lambda_i = 0$ by assumption.
		      Hence, $\lambda_1 = \dots = \lambda_n = 0$, so $w = 0$, implying $\#$ is injective.
	      \end{solution}
\end{enumerate}
\end{problem}

\begin{problem}
\begin{enumerate}[label=(\alph*)]
	\item Write the permutation $\sigma = \begin{pmatrix}
			      1 & 2 & 3 & 4 & 5 \\
			      2 & 3 & 1 & 5 & 4 \\
		      \end{pmatrix}$ as a product of transpositions.
	      \begin{solution}
		      \[
			      (1 2)(2 3)(4 5) = \left\{\begin{aligned}
				      1 \mapsto 1 \mapsto 1 \mapsto 2 \\
				      2 \mapsto 2 \mapsto 3 \mapsto 3 \\
				      3 \mapsto 3 \mapsto 2 \mapsto 1 \\
				      4 \mapsto 5 \mapsto 5 \mapsto 5 \\
				      5 \mapsto 4 \mapsto 4 \mapsto 4 \\
			      \end{aligned}\right. \equiv \begin{pmatrix}
				      1 & 2 & 3 & 4 & 5 \\
				      2 & 3 & 1 & 5 & 4 \\
			      \end{pmatrix}
		      \]
	      \end{solution}

	\item Compute the sign of the permutation $\sigma$.
	      \begin{solution}
		      We count the number of inversions in $\sigma$.
		      \begin{center}
			      \begin{tikzpicture}
				      \matrix[%
					      matrix of math nodes,
					      left delimiter=(,
					      right delimiter=),
					      column sep = 1em,
					      row sep = 1em
				      ] (sigma) {%
					      1 & 2 & 3 & 4 & 5 \\
					      1 & 2 & 3 & 4 & 5 \\
				      };

				      \draw[] (sigma-1-1.south) -- (sigma-2-3.north);
				      \draw[] (sigma-1-2.south) -- (sigma-2-1.north);
				      \draw[] (sigma-1-3.south) -- (sigma-2-2.north);
				      \draw[] (sigma-1-4.south) -- (sigma-2-5.north);
				      \draw[] (sigma-1-5.south) -- (sigma-2-4.north);
			      \end{tikzpicture}
		      \end{center}
		      Thus, the sign of $\sigma$ is $(-1)^3 = -1$.
	      \end{solution}
\end{enumerate}
\end{problem}

\begin{problem}
Recall that an inversion of a permutation $\sigma$ is a pair of indices $i, j$ so that $i < j$ and $\sigma(i) > \sigma(j)$.
The inversion number of $\sigma$ is the number of all inversions of $\sigma$.
What is the inversion number of
\[
	\sigma = \begin{pmatrix}
		1 & 2 & 3 & 4 \\
		2 & 3 & 4 & 1 \\
	\end{pmatrix}
\]
\end{problem}
\begin{solution}
	We count the crossings in $\sigma$.
	\begin{center}
		\begin{tikzpicture}
			\matrix[%
				matrix of math nodes,
				left delimiter=(,
				right delimiter=),
				column sep = 1em,
				row sep = 1em
			] (sigma) {%
				1 & 2 & 3 & 4 \\
				1 & 2 & 3 & 4 \\
			};

			\draw[] (sigma-1-1.south) -- (sigma-2-4.north);
			\draw[] (sigma-1-2.south) -- (sigma-2-1.north);
			\draw[] (sigma-1-3.south) -- (sigma-2-2.north);
			\draw[] (sigma-1-4.south) -- (sigma-2-3.north);
		\end{tikzpicture}
	\end{center}
	So, the inversion number of $\sigma$ is 3.
\end{solution}

\begin{problem}
Let $T : V \to W$ be a linear map and $\alpha : \overbrace{W \times \dots \times W}^{k} \to \R$ be $k$-linear and alternating.
Define
\[
	T^* \alpha : \overbrace{V \times \dots \times V}^{k} \to \R
\]
by
\[
	(T^* \alpha)(v_1, \dots, v_k) \coloneq \alpha (T(v_1), \dots, T(v_k)) \qquad \text{for all $v_1, \dots, v_k \in V$}.
\]
Prove that $T^* \alpha$ is $k$-linear and alternating.
\end{problem}
\begin{solution}
	Let $\lambda, \mu \in \R$, $u, v \in V$, and $1 \le i \le k$.
	Then,
	\begin{align*}
		T^* \alpha (v_1, \dots, \overbrace{\lambda u + \mu v}^{\text{$i$th slot}}, \dots, v_k) & = \alpha (T(v_1), \dots, \lambda T(u) + \mu T(v), \dots, T(v_k))                                                                     \\
		                                                                                       & = \lambda \alpha (T(v_1), \dots, T(u), \dots, T(v_k)) + \mu \alpha (T(v_1), \dots, T(v), \dots, T(v_k)) \tag{$\alpha$ is $k$-linear} \\
		                                                                                       & = \lambda T^* \alpha (v_1, \dots, u, \dots, v_k) + \mu T^* \alpha (v_1, \dots, v, \dots, v_k)
	\end{align*}
	Hence, $T^* \alpha$ is $k$-linear.

	Similarly, for $1 \le i < j \le k$, we have
	\begin{align*}
		T^* \alpha (v_1, \dots, \overbrace{v_j}^{\text{$i$th slot}}, \dots, \overbrace{v_i}^{\text{$j$th slot}}, \dots, v_k) & = \alpha(T(v_1), \dots, T(v_j), \dots, T(v_i), \dots, v_k)                                \\
		                                                                                                                     & = - \alpha T(v_1, \dots, T(v_i), \dots, T(v_j), \dots, v_k) \tag{$\alpha$ is alternating} \\
		                                                                                                                     & = - T(v_1, \dots, v_i, \dots, v_j, \dots, v_k)
	\end{align*}
	So, $T^* \alpha$ is alternating.
\end{solution}

\end{document}
